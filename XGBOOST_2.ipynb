{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>georgia  downs two russian warplanes  as count...</td>\n",
       "      <td>breaking  musharraf to be impeached</td>\n",
       "      <td>russia today  columns of troops roll into sout...</td>\n",
       "      <td>russian tanks are moving towards the capital o...</td>\n",
       "      <td>afghan children raped with  impunity   u n  of...</td>\n",
       "      <td>russian tanks have entered south ossetia w...</td>\n",
       "      <td>breaking  georgia invades south ossetia  russi...</td>\n",
       "      <td>the  enemy combatent  trials are nothing but a...</td>\n",
       "      <td>...</td>\n",
       "      <td>georgia invades south ossetia   if russia gets...</td>\n",
       "      <td>al qaeda faces islamist backlash</td>\n",
       "      <td>condoleezza rice   the us would not act to pre...</td>\n",
       "      <td>this is a busy day   the european union has ap...</td>\n",
       "      <td>georgia will withdraw       soldiers from iraq...</td>\n",
       "      <td>why the pentagon thinks attacking iran is a ba...</td>\n",
       "      <td>caucasus in crisis  georgia invades south osse...</td>\n",
       "      <td>indian shoe manufactory    and again in a seri...</td>\n",
       "      <td>visitors suffering from mental illnesses banne...</td>\n",
       "      <td>no help for mexico s kidnapping surge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-08-11</td>\n",
       "      <td>1</td>\n",
       "      <td>why wont america and nato help us  if they won...</td>\n",
       "      <td>bush puts foot down on georgian conflict</td>\n",
       "      <td>jewish georgian minister  thanks to israeli tr...</td>\n",
       "      <td>georgian army flees in disarray as russians ad...</td>\n",
       "      <td>olympic opening ceremony fireworks  faked</td>\n",
       "      <td>what were the mossad with fraudulent new zeala...</td>\n",
       "      <td>russia angered by israeli military sale to geo...</td>\n",
       "      <td>an american citizen living in s ossetia blames...</td>\n",
       "      <td>...</td>\n",
       "      <td>israel and the us behind the georgian aggressi...</td>\n",
       "      <td>do not believe tv  neither russian nor georgi...</td>\n",
       "      <td>riots are still going on in montreal  canada  ...</td>\n",
       "      <td>china to overtake us as largest manufacturer</td>\n",
       "      <td>war in south ossetia  pics</td>\n",
       "      <td>israeli physicians group condemns state torture</td>\n",
       "      <td>russia has just beaten the united states over...</td>\n",
       "      <td>perhaps  the  question about the georgia   rus...</td>\n",
       "      <td>russia is so much better at war</td>\n",
       "      <td>so this is what it s come to  trading sex for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-08-12</td>\n",
       "      <td>0</td>\n",
       "      <td>remember that adorable   year old who sang at ...</td>\n",
       "      <td>russia  ends georgia operation</td>\n",
       "      <td>if we had no sexual harassment we would have ...</td>\n",
       "      <td>al qa eda is losing support in iraq because of...</td>\n",
       "      <td>ceasefire in georgia  putin outmaneuvers the w...</td>\n",
       "      <td>why microsoft and intel tried to kill the xo  ...</td>\n",
       "      <td>stratfor  the russo georgian war and the balan...</td>\n",
       "      <td>i m trying to get a sense of this whole georgi...</td>\n",
       "      <td>...</td>\n",
       "      <td>u s  troops still in georgia  did you know the...</td>\n",
       "      <td>why russias response to georgia was right</td>\n",
       "      <td>gorbachev accuses u s  of making a  serious bl...</td>\n",
       "      <td>russia  georgia  and nato  cold war two</td>\n",
       "      <td>remember that adorable    year old who led you...</td>\n",
       "      <td>war in georgia  the israeli connection</td>\n",
       "      <td>all signs point to the us encouraging georgia ...</td>\n",
       "      <td>christopher king argues that the us and nato a...</td>\n",
       "      <td>america  the new mexico</td>\n",
       "      <td>bbc news   asia pacific   extinction  by man n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-08-13</td>\n",
       "      <td>0</td>\n",
       "      <td>u s  refuses israel weapons to attack iran  r...</td>\n",
       "      <td>when the president ordered to attack tskhinval...</td>\n",
       "      <td>israel clears troops who killed reuters camer...</td>\n",
       "      <td>britain  s policy of being tough on drugs is  ...</td>\n",
       "      <td>body of    year old found in trunk  latest  ra...</td>\n",
       "      <td>china has moved     million  quake survivors i...</td>\n",
       "      <td>bush announces operation get all up in russia ...</td>\n",
       "      <td>russian forces sink georgian ships</td>\n",
       "      <td>...</td>\n",
       "      <td>elephants extinct by</td>\n",
       "      <td>us humanitarian missions soon in georgia   if ...</td>\n",
       "      <td>georgia s ddos came from us sources</td>\n",
       "      <td>russian convoy heads into georgia  violating t...</td>\n",
       "      <td>israeli defence minister  us against strike on...</td>\n",
       "      <td>gorbachev  we had no choice</td>\n",
       "      <td>witness  russian forces head towards tbilisi i...</td>\n",
       "      <td>quarter of russians blame u s  for conflict  ...</td>\n",
       "      <td>georgian president  says us military will take...</td>\n",
       "      <td>nobel laureate aleksander solzhenitsyn a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-08-14</td>\n",
       "      <td>1</td>\n",
       "      <td>all the experts admit that we should legalise ...</td>\n",
       "      <td>war in south osetia      pictures made by a ru...</td>\n",
       "      <td>swedish wrestler ara abrahamian throws away me...</td>\n",
       "      <td>russia exaggerated the death toll in south oss...</td>\n",
       "      <td>missile that killed   inside pakistan may have...</td>\n",
       "      <td>rushdie condemns random house s refusal to pub...</td>\n",
       "      <td>poland and us agree to missle defense deal  in...</td>\n",
       "      <td>will the russians conquer tblisi  bet on it  n...</td>\n",
       "      <td>...</td>\n",
       "      <td>bank analyst forecast georgian crisis   days e...</td>\n",
       "      <td>georgia confict could set back russia s us rel...</td>\n",
       "      <td>war in the caucasus is as much the product of ...</td>\n",
       "      <td>non media  photos of south ossetia georgia co...</td>\n",
       "      <td>georgian tv reporter shot by russian sniper du...</td>\n",
       "      <td>saudi arabia  mother moves to block child marr...</td>\n",
       "      <td>taliban wages war on humanitarian aid workers</td>\n",
       "      <td>russia  world   can forget about  georgia  s t...</td>\n",
       "      <td>darfur rebels accuse sudan of mounting major a...</td>\n",
       "      <td>philippines   peace advocate say muslims need ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Label                                               Top1  \\\n",
       "0  2008-08-08      0  georgia  downs two russian warplanes  as count...   \n",
       "1  2008-08-11      1  why wont america and nato help us  if they won...   \n",
       "2  2008-08-12      0  remember that adorable   year old who sang at ...   \n",
       "3  2008-08-13      0   u s  refuses israel weapons to attack iran  r...   \n",
       "4  2008-08-14      1  all the experts admit that we should legalise ...   \n",
       "\n",
       "                                                Top2  \\\n",
       "0              breaking  musharraf to be impeached     \n",
       "1          bush puts foot down on georgian conflict    \n",
       "2                   russia  ends georgia operation     \n",
       "3  when the president ordered to attack tskhinval...   \n",
       "4  war in south osetia      pictures made by a ru...   \n",
       "\n",
       "                                                Top3  \\\n",
       "0  russia today  columns of troops roll into sout...   \n",
       "1  jewish georgian minister  thanks to israeli tr...   \n",
       "2   if we had no sexual harassment we would have ...   \n",
       "3   israel clears troops who killed reuters camer...   \n",
       "4  swedish wrestler ara abrahamian throws away me...   \n",
       "\n",
       "                                                Top4  \\\n",
       "0  russian tanks are moving towards the capital o...   \n",
       "1  georgian army flees in disarray as russians ad...   \n",
       "2  al qa eda is losing support in iraq because of...   \n",
       "3  britain  s policy of being tough on drugs is  ...   \n",
       "4  russia exaggerated the death toll in south oss...   \n",
       "\n",
       "                                                Top5  \\\n",
       "0  afghan children raped with  impunity   u n  of...   \n",
       "1        olympic opening ceremony fireworks  faked     \n",
       "2  ceasefire in georgia  putin outmaneuvers the w...   \n",
       "3  body of    year old found in trunk  latest  ra...   \n",
       "4  missile that killed   inside pakistan may have...   \n",
       "\n",
       "                                                Top6  \\\n",
       "0      russian tanks have entered south ossetia w...   \n",
       "1  what were the mossad with fraudulent new zeala...   \n",
       "2  why microsoft and intel tried to kill the xo  ...   \n",
       "3  china has moved     million  quake survivors i...   \n",
       "4  rushdie condemns random house s refusal to pub...   \n",
       "\n",
       "                                                Top7  \\\n",
       "0  breaking  georgia invades south ossetia  russi...   \n",
       "1  russia angered by israeli military sale to geo...   \n",
       "2  stratfor  the russo georgian war and the balan...   \n",
       "3  bush announces operation get all up in russia ...   \n",
       "4  poland and us agree to missle defense deal  in...   \n",
       "\n",
       "                                                Top8  ...  \\\n",
       "0  the  enemy combatent  trials are nothing but a...  ...   \n",
       "1  an american citizen living in s ossetia blames...  ...   \n",
       "2  i m trying to get a sense of this whole georgi...  ...   \n",
       "3               russian forces sink georgian ships    ...   \n",
       "4  will the russians conquer tblisi  bet on it  n...  ...   \n",
       "\n",
       "                                               Top16  \\\n",
       "0  georgia invades south ossetia   if russia gets...   \n",
       "1  israel and the us behind the georgian aggressi...   \n",
       "2  u s  troops still in georgia  did you know the...   \n",
       "3                        elephants extinct by          \n",
       "4  bank analyst forecast georgian crisis   days e...   \n",
       "\n",
       "                                               Top17  \\\n",
       "0                  al qaeda faces islamist backlash    \n",
       "1   do not believe tv  neither russian nor georgi...   \n",
       "2         why russias response to georgia was right    \n",
       "3  us humanitarian missions soon in georgia   if ...   \n",
       "4  georgia confict could set back russia s us rel...   \n",
       "\n",
       "                                               Top18  \\\n",
       "0  condoleezza rice   the us would not act to pre...   \n",
       "1  riots are still going on in montreal  canada  ...   \n",
       "2  gorbachev accuses u s  of making a  serious bl...   \n",
       "3               georgia s ddos came from us sources    \n",
       "4  war in the caucasus is as much the product of ...   \n",
       "\n",
       "                                               Top19  \\\n",
       "0  this is a busy day   the european union has ap...   \n",
       "1      china to overtake us as largest manufacturer    \n",
       "2           russia  georgia  and nato  cold war two    \n",
       "3  russian convoy heads into georgia  violating t...   \n",
       "4   non media  photos of south ossetia georgia co...   \n",
       "\n",
       "                                               Top20  \\\n",
       "0  georgia will withdraw       soldiers from iraq...   \n",
       "1                       war in south ossetia  pics     \n",
       "2  remember that adorable    year old who led you...   \n",
       "3  israeli defence minister  us against strike on...   \n",
       "4  georgian tv reporter shot by russian sniper du...   \n",
       "\n",
       "                                               Top21  \\\n",
       "0  why the pentagon thinks attacking iran is a ba...   \n",
       "1   israeli physicians group condemns state torture    \n",
       "2            war in georgia  the israeli connection    \n",
       "3                       gorbachev  we had no choice    \n",
       "4  saudi arabia  mother moves to block child marr...   \n",
       "\n",
       "                                               Top22  \\\n",
       "0  caucasus in crisis  georgia invades south osse...   \n",
       "1   russia has just beaten the united states over...   \n",
       "2  all signs point to the us encouraging georgia ...   \n",
       "3  witness  russian forces head towards tbilisi i...   \n",
       "4     taliban wages war on humanitarian aid workers    \n",
       "\n",
       "                                               Top23  \\\n",
       "0  indian shoe manufactory    and again in a seri...   \n",
       "1  perhaps  the  question about the georgia   rus...   \n",
       "2  christopher king argues that the us and nato a...   \n",
       "3   quarter of russians blame u s  for conflict  ...   \n",
       "4  russia  world   can forget about  georgia  s t...   \n",
       "\n",
       "                                               Top24  \\\n",
       "0  visitors suffering from mental illnesses banne...   \n",
       "1                   russia is so much better at war    \n",
       "2                          america  the new mexico     \n",
       "3  georgian president  says us military will take...   \n",
       "4  darfur rebels accuse sudan of mounting major a...   \n",
       "\n",
       "                                               Top25  \n",
       "0             no help for mexico s kidnapping surge   \n",
       "1  so this is what it s come to  trading sex for ...  \n",
       "2  bbc news   asia pacific   extinction  by man n...  \n",
       "3        nobel laureate aleksander solzhenitsyn a...  \n",
       "4  philippines   peace advocate say muslims need ...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsData = pd.read_csv('Combined_News_DJIA.csv')\n",
    "#remove all non-alphabetic char from headlines.\n",
    "df = newsData.copy()\n",
    "df.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n",
    "for i in df.columns:\n",
    "    if i=='Date':\n",
    "        continue\n",
    "    if i=='Label':\n",
    "        continue\n",
    "    df[i] = df[i].str.lower()\n",
    "df['Date'] = newsData['Date']\n",
    "df.replace(\"^b \",\"\",regex=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date     0\n",
       "Label    0\n",
       "Top1     0\n",
       "Top2     0\n",
       "Top3     0\n",
       "Top4     0\n",
       "Top5     0\n",
       "Top6     0\n",
       "Top7     0\n",
       "Top8     0\n",
       "Top9     0\n",
       "Top10    0\n",
       "Top11    0\n",
       "Top12    0\n",
       "Top13    0\n",
       "Top14    0\n",
       "Top15    0\n",
       "Top16    0\n",
       "Top17    0\n",
       "Top18    0\n",
       "Top19    0\n",
       "Top20    0\n",
       "Top21    0\n",
       "Top22    0\n",
       "Top23    0\n",
       "Top24    0\n",
       "Top25    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove NA and replace with empty string\n",
    "df.fillna(' ', inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeCol(row):\n",
    "    return ' '.join(row['Top1':'Top25'])\n",
    "df['headlines'] = df.apply(mergeCol, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [georgia, downs, two, russian, warplanes, as, ...\n",
       "1    [why, wont, america, and, nato, help, us, if, ...\n",
       "2    [remember, that, adorable, year, old, who, san...\n",
       "3    [u, s, refuses, israel, weapons, to, attack, i...\n",
       "4    [all, the, experts, admit, that, we, should, l...\n",
       "Name: headlines, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing\n",
    "tokenized_text = df['headlines'].apply(lambda x: x.split())\n",
    "tokenized_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#Stemming\n",
    "tokenized_text = tokenized_text.apply(lambda x: [stemmer.stem(i)\n",
    "                                                  for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(tokenized_text)):\n",
    "    tokenized_text[i] = ' '.join(tokenized_text[i])\n",
    "df['tidy_text'] = tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in headline are 33270\n",
      "Total unique words in tidy text are 21809\n"
     ]
    }
   ],
   "source": [
    "all_headline_words = ' '.join([text for text in df['headlines']])\n",
    "print(f'Total unique words in headline are {len(set(all_headline_words.split()))}')\n",
    "\n",
    "all_tidytext_words = ' '.join([text for text in df['tidy_text']])\n",
    "print(f'Total unique words in tidy text are {len(set(all_tidytext_words.split()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=10000, ngram_range=(2, 2), stop_words='english')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorized the corpus on the whole data. Using unigram does not make sense, we can test bigrams or higher for better accuracy. Taking maximum features.\n",
    "\n",
    "countvector=CountVectorizer(ngram_range=(2,2),max_features=10000, stop_words = 'english') #todo play with max_feature and ngram\n",
    "countvector.fit(df['headlines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate the test and train data\n",
    "train = df[df['Date'] < '2015-01-01']\n",
    "test = df[df['Date'] > '2014-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the data\n",
    "#X_train = countvector.transform(train['headlines'])\n",
    "#X_test = countvector.transform(test['headlines'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to find best max_feature and ngram for CountVectorizer on headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\j88796\\AppData\\Local\\Continuum\\anaconda3\\envs\\poa\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:39:34] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 500\n",
      "ngram_range (1,1)\n",
      "0.5\n",
      "confusion matrix : [[ 71 115]\n",
      " [ 74 118]]\n",
      "===============================\n",
      "[18:39:44] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 500\n",
      "ngram_range (2,2)\n",
      "0.48677248677248675\n",
      "confusion matrix : [[ 69 117]\n",
      " [ 77 115]]\n",
      "===============================\n",
      "[18:39:55] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 500\n",
      "ngram_range (3,3)\n",
      "0.5555555555555556\n",
      "confusion matrix : [[ 57 129]\n",
      " [ 39 153]]\n",
      "===============================\n",
      "[18:40:08] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 500\n",
      "ngram_range (4,4)\n",
      "0.5238095238095238\n",
      "confusion matrix : [[ 15 171]\n",
      " [  9 183]]\n",
      "===============================\n",
      "[18:40:19] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 500\n",
      "ngram_range (5,5)\n",
      "0.5052910052910053\n",
      "confusion matrix : [[  1 185]\n",
      " [  2 190]]\n",
      "===============================\n",
      "[18:40:25] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 600\n",
      "ngram_range (1,1)\n",
      "0.4523809523809524\n",
      "confusion matrix : [[ 66 120]\n",
      " [ 87 105]]\n",
      "===============================\n",
      "[18:40:35] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 600\n",
      "ngram_range (2,2)\n",
      "0.5529100529100529\n",
      "confusion matrix : [[ 85 101]\n",
      " [ 68 124]]\n",
      "===============================\n",
      "[18:40:43] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 600\n",
      "ngram_range (3,3)\n",
      "0.5582010582010583\n",
      "confusion matrix : [[ 59 127]\n",
      " [ 40 152]]\n",
      "===============================\n",
      "[18:40:55] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 600\n",
      "ngram_range (4,4)\n",
      "0.5238095238095238\n",
      "confusion matrix : [[ 15 171]\n",
      " [  9 183]]\n",
      "===============================\n",
      "[18:41:08] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 600\n",
      "ngram_range (5,5)\n",
      "0.5052910052910053\n",
      "confusion matrix : [[  1 185]\n",
      " [  2 190]]\n",
      "===============================\n",
      "[18:41:13] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 700\n",
      "ngram_range (1,1)\n",
      "0.5291005291005291\n",
      "confusion matrix : [[ 71 115]\n",
      " [ 63 129]]\n",
      "===============================\n",
      "[18:41:25] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 700\n",
      "ngram_range (2,2)\n",
      "0.5317460317460317\n",
      "confusion matrix : [[ 80 106]\n",
      " [ 71 121]]\n",
      "===============================\n",
      "[18:41:38] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 700\n",
      "ngram_range (3,3)\n",
      "0.5502645502645502\n",
      "confusion matrix : [[ 52 134]\n",
      " [ 36 156]]\n",
      "===============================\n",
      "[18:41:49] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 700\n",
      "ngram_range (4,4)\n",
      "0.5238095238095238\n",
      "confusion matrix : [[ 15 171]\n",
      " [  9 183]]\n",
      "===============================\n",
      "[18:42:02] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 700\n",
      "ngram_range (5,5)\n",
      "0.5052910052910053\n",
      "confusion matrix : [[  1 185]\n",
      " [  2 190]]\n",
      "===============================\n",
      "[18:42:10] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 800\n",
      "ngram_range (1,1)\n",
      "0.5105820105820106\n",
      "confusion matrix : [[ 86 100]\n",
      " [ 85 107]]\n",
      "===============================\n",
      "[18:42:21] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 800\n",
      "ngram_range (2,2)\n",
      "0.5052910052910053\n",
      "confusion matrix : [[ 71 115]\n",
      " [ 72 120]]\n",
      "===============================\n",
      "[18:42:33] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 800\n",
      "ngram_range (3,3)\n",
      "0.5502645502645502\n",
      "confusion matrix : [[ 52 134]\n",
      " [ 36 156]]\n",
      "===============================\n",
      "[18:42:46] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 800\n",
      "ngram_range (4,4)\n",
      "0.5238095238095238\n",
      "confusion matrix : [[ 15 171]\n",
      " [  9 183]]\n",
      "===============================\n",
      "[18:42:57] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 800\n",
      "ngram_range (5,5)\n",
      "0.5052910052910053\n",
      "confusion matrix : [[  1 185]\n",
      " [  2 190]]\n",
      "===============================\n",
      "[18:43:05] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 900\n",
      "ngram_range (1,1)\n",
      "0.4708994708994709\n",
      "confusion matrix : [[ 74 112]\n",
      " [ 88 104]]\n",
      "===============================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:43:17] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 900\n",
      "ngram_range (2,2)\n",
      "0.5158730158730159\n",
      "confusion matrix : [[ 82 104]\n",
      " [ 79 113]]\n",
      "===============================\n",
      "[18:43:30] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 900\n",
      "ngram_range (3,3)\n",
      "0.5502645502645502\n",
      "confusion matrix : [[ 52 134]\n",
      " [ 36 156]]\n",
      "===============================\n",
      "[18:43:43] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 900\n",
      "ngram_range (4,4)\n",
      "0.5238095238095238\n",
      "confusion matrix : [[ 15 171]\n",
      " [  9 183]]\n",
      "===============================\n",
      "[18:43:56] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 900\n",
      "ngram_range (5,5)\n",
      "0.5052910052910053\n",
      "confusion matrix : [[  1 185]\n",
      " [  2 190]]\n",
      "===============================\n",
      "[18:44:04] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 1000\n",
      "ngram_range (1,1)\n",
      "0.47883597883597884\n",
      "confusion matrix : [[ 65 121]\n",
      " [ 76 116]]\n",
      "===============================\n",
      "[18:44:16] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 1000\n",
      "ngram_range (2,2)\n",
      "0.5185185185185185\n",
      "confusion matrix : [[ 80 106]\n",
      " [ 76 116]]\n",
      "===============================\n",
      "[18:44:30] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 1000\n",
      "ngram_range (3,3)\n",
      "0.5502645502645502\n",
      "confusion matrix : [[ 52 134]\n",
      " [ 36 156]]\n",
      "===============================\n",
      "[18:44:42] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 1000\n",
      "ngram_range (4,4)\n",
      "0.5238095238095238\n",
      "confusion matrix : [[ 15 171]\n",
      " [  9 183]]\n",
      "===============================\n",
      "[18:44:55] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 1000\n",
      "ngram_range (5,5)\n",
      "0.5052910052910053\n",
      "confusion matrix : [[  1 185]\n",
      " [  2 190]]\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "max_features_num = [500,600,700,800,900,1000]\n",
    "ngram = [1,2,3,4,5]\n",
    "scores_df = pd.DataFrame(np.zeros((len(max_features_num),len(max_features_num))))\n",
    "for i in range(len(max_features_num)):\n",
    "    for j in ngram:\n",
    "        countvector=CountVectorizer(ngram_range=(j,j),max_features=max_features_num[i],stop_words = 'english')\n",
    "        countvector.fit(df['headlines'])\n",
    "        X_train = countvector.transform(train['headlines'])\n",
    "        X_test = countvector.transform(test['headlines'])\n",
    "        #traindataset=countvector.fit_transform(headlines)\n",
    "        #test_dataset = countvector.transform(test_transform)\n",
    "\n",
    "        xgb = XGBClassifier(random_state =1)\n",
    "        xgb.fit(pd.DataFrame(X_train.todense(), columns=countvector.get_feature_names()),train['Label'])\n",
    "        predictions = xgb.predict(pd.DataFrame(X_test.todense(), columns=countvector.get_feature_names()))\n",
    "        score=accuracy_score(test['Label'],predictions)\n",
    "        print('max number of features used : {}'.format(max_features_num[i]))\n",
    "        print('ngram_range ({},{})'.format(j,j))\n",
    "        print(score)\n",
    "        matrix=confusion_matrix(test['Label'],predictions)\n",
    "        print('confusion matrix : {}'.format(matrix))\n",
    "        print('===============================')\n",
    "        \n",
    "        scores_df.iloc[j,i] = score\n",
    "        \n",
    "#Rename column titles to max_feature_num\n",
    "scores_df.columns = max_features_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>500</th>\n",
       "      <th>600</th>\n",
       "      <th>700</th>\n",
       "      <th>800</th>\n",
       "      <th>900</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.529101</td>\n",
       "      <td>0.510582</td>\n",
       "      <td>0.470899</td>\n",
       "      <td>0.478836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.486772</td>\n",
       "      <td>0.552910</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.505291</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>0.518519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.558201</td>\n",
       "      <td>0.550265</td>\n",
       "      <td>0.550265</td>\n",
       "      <td>0.550265</td>\n",
       "      <td>0.550265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.523810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.505291</td>\n",
       "      <td>0.505291</td>\n",
       "      <td>0.505291</td>\n",
       "      <td>0.505291</td>\n",
       "      <td>0.505291</td>\n",
       "      <td>0.505291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       500       600       700       800       900       1000\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "1  0.500000  0.452381  0.529101  0.510582  0.470899  0.478836\n",
       "2  0.486772  0.552910  0.531746  0.505291  0.515873  0.518519\n",
       "3  0.555556  0.558201  0.550265  0.550265  0.550265  0.550265\n",
       "4  0.523810  0.523810  0.523810  0.523810  0.523810  0.523810\n",
       "5  0.505291  0.505291  0.505291  0.505291  0.505291  0.505291"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Max Feature_num vs ngram range\n",
    "\n",
    "scores_df\n",
    "#Highest accuracy resides with ngram (1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\j88796\\AppData\\Local\\Continuum\\anaconda3\\envs\\poa\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:04] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 500\n",
      "ngram_range (1,1)\n",
      "0.4708994708994709\n",
      "confusion matrix : [[ 72 114]\n",
      " [ 86 106]]\n",
      "===============================\n",
      "[18:45:13] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 500\n",
      "ngram_range (2,2)\n",
      "0.5079365079365079\n",
      "confusion matrix : [[ 72 114]\n",
      " [ 72 120]]\n",
      "===============================\n",
      "[18:45:24] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 500\n",
      "ngram_range (3,3)\n",
      "0.5211640211640212\n",
      "confusion matrix : [[ 48 138]\n",
      " [ 43 149]]\n",
      "===============================\n",
      "[18:45:35] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 500\n",
      "ngram_range (4,4)\n",
      "0.5079365079365079\n",
      "confusion matrix : [[ 12 174]\n",
      " [ 12 180]]\n",
      "===============================\n",
      "[18:45:48] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 500\n",
      "ngram_range (5,5)\n",
      "0.5052910052910053\n",
      "confusion matrix : [[  1 185]\n",
      " [  2 190]]\n",
      "===============================\n",
      "[18:45:53] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 600\n",
      "ngram_range (1,1)\n",
      "0.4894179894179894\n",
      "confusion matrix : [[ 73 113]\n",
      " [ 80 112]]\n",
      "===============================\n",
      "[18:46:05] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 600\n",
      "ngram_range (2,2)\n",
      "0.48412698412698413\n",
      "confusion matrix : [[ 68 118]\n",
      " [ 77 115]]\n",
      "===============================\n",
      "[18:46:18] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 600\n",
      "ngram_range (3,3)\n",
      "0.5317460317460317\n",
      "confusion matrix : [[ 53 133]\n",
      " [ 44 148]]\n",
      "===============================\n",
      "[18:46:29] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 600\n",
      "ngram_range (4,4)\n",
      "0.5079365079365079\n",
      "confusion matrix : [[ 12 174]\n",
      " [ 12 180]]\n",
      "===============================\n",
      "[18:46:36] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 600\n",
      "ngram_range (5,5)\n",
      "0.5052910052910053\n",
      "confusion matrix : [[  1 185]\n",
      " [  2 190]]\n",
      "===============================\n",
      "[18:46:42] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 700\n",
      "ngram_range (1,1)\n",
      "0.47619047619047616\n",
      "confusion matrix : [[ 69 117]\n",
      " [ 81 111]]\n",
      "===============================\n",
      "[18:46:51] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 700\n",
      "ngram_range (2,2)\n",
      "0.5132275132275133\n",
      "confusion matrix : [[ 68 118]\n",
      " [ 66 126]]\n",
      "===============================\n",
      "[18:47:00] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 700\n",
      "ngram_range (3,3)\n",
      "0.5370370370370371\n",
      "confusion matrix : [[ 54 132]\n",
      " [ 43 149]]\n",
      "===============================\n",
      "[18:47:09] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 700\n",
      "ngram_range (4,4)\n",
      "0.5079365079365079\n",
      "confusion matrix : [[ 12 174]\n",
      " [ 12 180]]\n",
      "===============================\n",
      "[18:47:19] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 700\n",
      "ngram_range (5,5)\n",
      "0.5052910052910053\n",
      "confusion matrix : [[  1 185]\n",
      " [  2 190]]\n",
      "===============================\n",
      "[18:47:24] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 800\n",
      "ngram_range (1,1)\n",
      "0.47883597883597884\n",
      "confusion matrix : [[ 69 117]\n",
      " [ 80 112]]\n",
      "===============================\n",
      "[18:47:33] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 800\n",
      "ngram_range (2,2)\n",
      "0.5158730158730159\n",
      "confusion matrix : [[ 73 113]\n",
      " [ 70 122]]\n",
      "===============================\n",
      "[18:47:43] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 800\n",
      "ngram_range (3,3)\n",
      "0.5423280423280423\n",
      "confusion matrix : [[ 56 130]\n",
      " [ 43 149]]\n",
      "===============================\n",
      "[18:47:53] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 800\n",
      "ngram_range (4,4)\n",
      "0.5079365079365079\n",
      "confusion matrix : [[ 12 174]\n",
      " [ 12 180]]\n",
      "===============================\n",
      "[18:48:04] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 800\n",
      "ngram_range (5,5)\n",
      "0.5052910052910053\n",
      "confusion matrix : [[  1 185]\n",
      " [  2 190]]\n",
      "===============================\n",
      "[18:48:10] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max number of features used : 900\n",
      "ngram_range (1,1)\n",
      "0.4894179894179894\n",
      "confusion matrix : [[ 69 117]\n",
      " [ 76 116]]\n",
      "===============================\n",
      "[18:48:20] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 900\n",
      "ngram_range (2,2)\n",
      "0.5132275132275133\n",
      "confusion matrix : [[ 79 107]\n",
      " [ 77 115]]\n",
      "===============================\n",
      "[18:48:29] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 900\n",
      "ngram_range (3,3)\n",
      "0.5396825396825397\n",
      "confusion matrix : [[ 52 134]\n",
      " [ 40 152]]\n",
      "===============================\n",
      "[18:48:39] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 900\n",
      "ngram_range (4,4)\n",
      "0.5079365079365079\n",
      "confusion matrix : [[ 12 174]\n",
      " [ 12 180]]\n",
      "===============================\n",
      "[18:48:49] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 900\n",
      "ngram_range (5,5)\n",
      "0.5052910052910053\n",
      "confusion matrix : [[  1 185]\n",
      " [  2 190]]\n",
      "===============================\n",
      "[18:48:55] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 1000\n",
      "ngram_range (1,1)\n",
      "0.5132275132275133\n",
      "confusion matrix : [[ 71 115]\n",
      " [ 69 123]]\n",
      "===============================\n",
      "[18:49:04] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 1000\n",
      "ngram_range (2,2)\n",
      "0.5185185185185185\n",
      "confusion matrix : [[ 74 112]\n",
      " [ 70 122]]\n",
      "===============================\n",
      "[18:49:14] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 1000\n",
      "ngram_range (3,3)\n",
      "0.5502645502645502\n",
      "confusion matrix : [[ 56 130]\n",
      " [ 40 152]]\n",
      "===============================\n",
      "[18:49:24] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 1000\n",
      "ngram_range (4,4)\n",
      "0.5079365079365079\n",
      "confusion matrix : [[ 12 174]\n",
      " [ 12 180]]\n",
      "===============================\n",
      "[18:49:34] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "max number of features used : 1000\n",
      "ngram_range (5,5)\n",
      "0.5052910052910053\n",
      "confusion matrix : [[  1 185]\n",
      " [  2 190]]\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "max_features_num = [500,600,700,800,900,1000]\n",
    "ngram = [1,2,3,4,5]\n",
    "scores_df_tt = pd.DataFrame(np.zeros((len(max_features_num),len(max_features_num))))\n",
    "for i in range(len(max_features_num)):\n",
    "    for j in ngram:\n",
    "        countvector=CountVectorizer(ngram_range=(j,j),max_features=max_features_num[i],stop_words = 'english')\n",
    "        countvector.fit(df['tidy_text'])\n",
    "        X_train = countvector.transform(train['tidy_text'])\n",
    "        X_test = countvector.transform(test['tidy_text'])\n",
    "        #traindataset=countvector.fit_transform(headlines)\n",
    "        #test_dataset = countvector.transform(test_transform)\n",
    "\n",
    "        xgb = XGBClassifier(random_state =1)\n",
    "        xgb.fit(pd.DataFrame(X_train.todense(), columns=countvector.get_feature_names()),train['Label'])\n",
    "        predictions = xgb.predict(pd.DataFrame(X_test.todense(), columns=countvector.get_feature_names()))\n",
    "        score=accuracy_score(test['Label'],predictions)\n",
    "        print('max number of features used : {}'.format(max_features_num[i]))\n",
    "        print('ngram_range ({},{})'.format(j,j))\n",
    "        print(score)\n",
    "        matrix=confusion_matrix(test['Label'],predictions)\n",
    "        print('confusion matrix : {}'.format(matrix))\n",
    "        print('===============================')\n",
    "        \n",
    "        scores_df_tt.iloc[j,i] = score\n",
    "        \n",
    "#Rename column titles to max_feature_num\n",
    "scores_df_tt.columns = max_features_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>500</th>\n",
       "      <th>600</th>\n",
       "      <th>700</th>\n",
       "      <th>800</th>\n",
       "      <th>900</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.470899</td>\n",
       "      <td>0.489418</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.478836</td>\n",
       "      <td>0.489418</td>\n",
       "      <td>0.513228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.484127</td>\n",
       "      <td>0.513228</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>0.513228</td>\n",
       "      <td>0.518519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.521164</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.542328</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.550265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.507937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.505291</td>\n",
       "      <td>0.505291</td>\n",
       "      <td>0.505291</td>\n",
       "      <td>0.505291</td>\n",
       "      <td>0.505291</td>\n",
       "      <td>0.505291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       500       600       700       800       900       1000\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "1  0.470899  0.489418  0.476190  0.478836  0.489418  0.513228\n",
       "2  0.507937  0.484127  0.513228  0.515873  0.513228  0.518519\n",
       "3  0.521164  0.531746  0.537037  0.542328  0.539683  0.550265\n",
       "4  0.507937  0.507937  0.507937  0.507937  0.507937  0.507937\n",
       "5  0.505291  0.505291  0.505291  0.505291  0.505291  0.505291"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Max Feature_num vs ngram range\n",
    "\n",
    "scores_df_tt\n",
    "#Highest accuracy still resides with ngram (1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST Hyperparameters\n",
    "Accuracy seemed to be highest on tidy text over headlines. ngram (1,1) seems highest overall. And we'll keep max features to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-42c70bbf9778>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mclf_xgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distributions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mbest_clf_xgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_xgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcountvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\poa\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\poa\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\poa\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1529\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1530\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1531\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\poa\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\poa\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\poa\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\poa\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\poa\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\poa\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "countvector=CountVectorizer(ngram_range=(1,1),max_features=1000,stop_words = 'english')\n",
    "countvector.fit(df['tidy_text'])\n",
    "X_train = countvector.transform(train['tidy_text'])\n",
    "X_test = countvector.transform(test['tidy_text'])\n",
    "\n",
    "xgb = XGBClassifier(random_state =1)\n",
    "param_grid = {\n",
    "    'n_estimators': [500,550,600,650],\n",
    "    'colsample_bytree': [0.75,0.8,0.85],\n",
    "    'max_depth': [None],\n",
    "    'reg_alpha': [1],\n",
    "    'reg_lambda': [2, 5, 10],\n",
    "    'subsample': [0.55, 0.6, .65,0.9],\n",
    "    'learning_rate':[0.5],\n",
    "    'gamma':[.5,1,2],\n",
    "    'min_child_weight':[0.01],\n",
    "    'sampling_method': ['uniform']\n",
    "}\n",
    "\n",
    "clf_xgb = RandomizedSearchCV(xgb, param_distributions = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_xgb = clf_xgb.fit(pd.DataFrame(X_train.todense(), columns=countvector.get_feature_names()),train['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_clf_xgb.best_score_)\n",
    "print(best_clf_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#best_clf_xgb = best_clf_xgb.best_estimator_\n",
    "\n",
    "best_clf_xgb.fit(pd.DataFrame(X_train.todense(), columns=countvector.get_feature_names()),train['Label'])\n",
    "predictions = best_clf_xgb.predict(pd.DataFrame(X_test.todense(), columns=countvector.get_feature_names()))\n",
    "score=accuracy_score(test['Label'],predictions)\n",
    "print(score)\n",
    "matrix=confusion_matrix(test['Label'],predictions)\n",
    "print('confusion matrix :')\n",
    "matrix=confusion_matrix(test['Label'],predictions)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try CatBoost on same countvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb=CatBoostClassifier(random_state=1)\n",
    "cb.fit(pd.DataFrame(X_train.todense(), columns=countvector.get_feature_names()),train['Label'])\n",
    "predictions = cb.predict(pd.DataFrame(X_test.todense(), columns=countvector.get_feature_names()))\n",
    "matrix=confusion_matrix(test['Label'],predictions)\n",
    "score=accuracy_score(test['Label'],predictions)\n",
    "print(score)\n",
    "print('===============')\n",
    "print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
